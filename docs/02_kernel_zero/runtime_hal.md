# Eidos Zero: Runtime HAL (The Backend)

## 1. The Hybrid Runtime Model

The Hardware Abstraction Layer (HAL) is responsible for physically executing the plan generated by the Compiler. Eidos implements a **Hybrid Runtime** that dynamically routes computation to the most appropriate "Lane".

### 1.1 The Vector Lane (Polars)
*   **Engine**: `eidos.backends.polars` (Rust).
*   **Use Case**: General-purpose ETL, Local processing (Single Node).
*   **Mechanism**: 
    *   Uses **Apache Arrow** for in-memory data representation.
    *   Executes vectorized kernels with SIMD acceleration.
    *   Performs "Out-of-Core" streaming for datasets larger than RAM.
*   **Performance**: Comparable to C++ hand-optimized code.

### 1.2 The Cluster Lane (Ray)
*   **Engine**: `eidos.backends.ray` (Distributed Python/C++).
*   **Use Case**: Massive Scale (TB/PB), Distributed Training.
*   **Mechanism**:
    *   Partitions the DAG into Stages.
    *   Schedules Tasks across a cluster of workers.
    *   Uses Ray's Object Store for zero-copy data sharing between processes.
*   **Auto-Scaling**: Supports dynamic worker allocation.

### 1.3 The Free Lane (Python 3.14 No-GIL)
*   **Engine**: `eidos.backends.python_native` (CPython 3.14+).
*   **Use Case**: Complex UDFs (User Defined Functions) that cannot be vectorized (e.g., calling a third-party library, recursion).
*   **Mechanism**:
    *   Runs raw Python code.
    *   **Key Innovation**: Disables the Global Interpreter Lock (GIL). This allows Python threads to run in true parallel on multi-core CPUs, removing the historical bottleneck of Python.

### 1.4 The Pushdown Lane (SQL)
*   **Engine**: `eidos.backends.dolphindb` / `eidos.backends.sql`.
*   **Use Case**: In-Database processing.
*   **Mechanism**:
    *   Sends SQL/Script to the database engine.
    *   Only retrieves the final result (e.g., a single number or a small summary table).
    *   **Zero Data Movement**: Petabytes of data are processed where they live.

### 1.5 The Triton Lane (GPU)
*   **Engine**: `eidos.backends.triton`.
*   **Use Case**: High-performance numerical computing, custom kernels.
*   **Mechanism**:
    *   Transpiles `Map` operations into **OpenAI Triton** kernels.
    *   JIT compiles kernels to PTX/CUDA.
    *   Executes on NVIDIA GPUs.

## 2. Backend Selection Strategy

How does Eidos choose the lane? It uses a cost-based heuristic:

1.  **Explicit Override**: `flow.run(engine="ray")` (Highest priority).
2.  **Data Locality**: If `Source` is a Database, prefer **Pushdown Lane**.
3.  **Data Volume**: If estimated size > RAM, prefer **Cluster Lane**.
4.  **Hardware**: If GPU is available and logic is numeric, prefer **Triton Lane**.
5.  **Logic Type**:
    *   If logic is standard relational algebra -> **Vector Lane**.
    *   If logic is opaque UDF -> **Free Lane**.

## 3. The Zero-Copy Promise

The HAL guarantees **Zero-Copy** wherever possible via the Apache Arrow standard.

### 3.1 Cross-Language
When passing data from Python (Free Lane) to Rust (Vector Lane), we use the `Arrow C Data Interface`. Only a pointer is passed; memory is not copied.

### 3.2 Cross-Process
When running in Ray (Cluster Lane), objects are stored in shared memory (Plasma). Different worker processes on the same node `mmap` the data, avoiding serialization overhead.

## 4. Extending the HAL

To add a new backend (e.g., DuckDB):
1.  Implement `Backend` protocol.
2.  Register capabilities via `entry_points`.
3.  Implement a `Transpiler` pass to convert Eidos IR to DuckDB SQL.
